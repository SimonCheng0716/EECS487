# EECS487: Exploring Gender Bias in LLM-Generated Job Descriptions

This project investigates potential gender biases in job descriptions generated by large language models (LLMs). By analyzing outputs from various LLMs, the study aims to understand how different prompt designs influence gender associations in generated content.

## Project Structure

- **Data/**
  - Contains `Occupation Data.xlsx` with 1,016 real-world job titles sourced from the O*NET database

- **Prompt/**:
  - `generate_prompts.py`:Script to generate prompts for all models and styles
  - `Vicuna/`, `Mistral_7B/`, `Llama2_7B/`: Directories containing prompts tailored for each respective model

- **Evaluation/**:
  - `gender_evaluation.py`:Script utilizing GPT-4o-mini for gender inference on generated descriptions

- **README.md**:Provides an overview and instructions for using the project

## Prompt Variants
The project employs four prompt variants to generate job description:

| Prompt Type    | Includes Job Title | Gender-Neutral |
|----------------|--------------------|----------------|
| `title_zero`   | ✅                 | ❌             |
| `title_one`    | ✅                 | ✅             |
| `wo_title_zero`| ❌                 | ❌             |
| `wo_title_one` | ❌                 | ✅             |
Each variant is designed to assess the impact of prompt structure on the gender associations in the generated conten.

## Evaluation Methodology

- **Assessment Tool**: GPT-4o-mini is used to infer the perceived gender from each generated job descriptin.

- **Metrics**: The analysis focuses on the distribution of gender associations (male, female, both) across different models and prompt typs.

## Visualization & Resuls

The project includes scripts to generate visualizations and tables that illustrate the gender distribution in the generated job descriptin. These outputs help in identifying patterns and biases present in the LLMs' outpts.

## Getting Started

1. **Clone the Repository**:
   ```bash
   git clone https://github.com/SimonCheng0716/EECS487.git
   cd EECS487
   ```

2. **Install Dependencies**:
   Ensure you have Python installed. Then, install the required packages:
   ```bash
   pip install -r requirements.txt
   ```

3. **Generate Prompts**:
   Run the prompt generation script:
   ```bash
   python Prompt/generate_prompts.py
   ```

4. **Run Evaluations**:
   Execute the gender evaluation script:
   ```bash
   python Evaluation/gender_evaluation.py
   ```

5. **Visualize Results**:
   Use the provided scripts to generate visualizations and analyze the results.

## Contribuing

Contributions are weclome! Please open an issue or submit a pull request for any enhancements or bug ixes.

## Liense

This project is licensed under the MIT Lcense.

